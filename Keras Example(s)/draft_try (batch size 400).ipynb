{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdl/.virtualenvs/test0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/mdl/.virtualenvs/test0/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/mdl/.virtualenvs/test0/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.8348 - acc: 0.5040 - val_loss: 0.7384 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.7025 - acc: 0.5180 - val_loss: 0.6921 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6888 - acc: 0.5420 - val_loss: 0.6917 - val_acc: 0.5325\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6864 - acc: 0.5495 - val_loss: 0.6831 - val_acc: 0.5688\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6850 - acc: 0.5620 - val_loss: 0.6838 - val_acc: 0.6413\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6777 - acc: 0.5945 - val_loss: 0.7605 - val_acc: 0.5175\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.7004 - acc: 0.5400 - val_loss: 0.6758 - val_acc: 0.5913\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6875 - acc: 0.5800 - val_loss: 0.6685 - val_acc: 0.5813\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6625 - acc: 0.5950 - val_loss: 0.7342 - val_acc: 0.5838\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6791 - acc: 0.5685 - val_loss: 0.7490 - val_acc: 0.5187\n"
     ]
    }
   ],
   "source": [
    "import os, os.path, re # ADDED, for dir operations\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "def numSamples(path):\n",
    "    dirName = path\n",
    "    # Get the list of all files in directory tree at given path\n",
    "    listOfFiles = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dirName):\n",
    "        listOfFiles += [os.path.join(dirpath, file) for file in filenames]\n",
    "        # Get the total count from the full list of all files in dir and subdir\n",
    "        imgsInDir = [f for f in listOfFiles if re.search(r'.*\\.(jpg)$', f)]\n",
    "    return (len(imgsInDir)) # len() is required to return integer, else full list.\n",
    "\n",
    "img_wh = 150 # 'global' variable\n",
    "img_width, img_height = img_wh, img_wh # reduce modification to the original example\n",
    "train_data_dir = './dogs-vs-cats/data1/train/' # should not require changing if dir setup properly\n",
    "validation_data_dir = './dogs-vs-cats/data1/validation/' # should not require changing if dir setup properly\n",
    "nb_train_samples = numSamples(train_data_dir) # ADDED\n",
    "nb_validation_samples = numSamples(validation_data_dir) # ADDED\n",
    "epochs = 10\n",
    "batch_size = 400 # batch size determines \n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "# convnet Block 1 layer 1\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# convnet Block 1 layer 2\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# convnet Block 1 layer 3\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Block 1 is connected to two fully-connected (fc) layers (flatten(fc1) and dense (fc2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid')) \n",
    "\n",
    "#Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('draft_try.h5') # always save after or during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
